{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import datetime\n",
    "# import lightgbm as lgb\n",
    "import scipy.stats as st\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import f1_score\n",
    "import hunspell\n",
    "pd.set_option('max_colwidth',400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_fe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Text</th>\n",
       "      <th>length</th>\n",
       "      <th>binnedLength</th>\n",
       "      <th>numWords</th>\n",
       "      <th>numCaps</th>\n",
       "      <th>numSpecial</th>\n",
       "      <th>numEmoticons</th>\n",
       "      <th>numMultiExclamations</th>\n",
       "      <th>numMultiStopMarks</th>\n",
       "      <th>...</th>\n",
       "      <th>mostSignificantSentimentScore</th>\n",
       "      <th>num_angerWords</th>\n",
       "      <th>num_anticipationWords</th>\n",
       "      <th>num_disgustWords</th>\n",
       "      <th>num_fearWords</th>\n",
       "      <th>num_joyWords</th>\n",
       "      <th>num_sadnessWords</th>\n",
       "      <th>num_surpriseWords</th>\n",
       "      <th>num_trustWords</th>\n",
       "      <th>linenumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>she had another song out recently but it did not get that much play</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>6</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>wellity! i think i ama gonna clean my room | gay. i need to read a good book. sense and sensibility here we come! beat the lily</td>\n",
       "      <td>0.418487</td>\n",
       "      <td>12</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>bug in damn cod system link. my map pack isnt working</td>\n",
       "      <td>-0.095211</td>\n",
       "      <td>5</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>weeooow, i feel fat multiStop  i remember when i used to weigh that much  pfffffffffffffffffffffftttttttt long ti ame ago, hahaha</td>\n",
       "      <td>0.377392</td>\n",
       "      <td>11</td>\n",
       "      <td>0.188525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>mmmm now you are speaking my language! (unfortunately, in militant diet mode-am seriously thinking of  competiton-so no tb i guess</td>\n",
       "      <td>0.432186</td>\n",
       "      <td>12</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.023077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  \\\n",
       "0          2   \n",
       "1          2   \n",
       "2          2   \n",
       "3          8   \n",
       "4          7   \n",
       "\n",
       "                                                                                                                                 Text  \\\n",
       "0                                                                 she had another song out recently but it did not get that much play   \n",
       "1     wellity! i think i ama gonna clean my room | gay. i need to read a good book. sense and sensibility here we come! beat the lily   \n",
       "2                                                                               bug in damn cod system link. my map pack isnt working   \n",
       "3   weeooow, i feel fat multiStop  i remember when i used to weigh that much  pfffffffffffffffffffffftttttttt long ti ame ago, hahaha   \n",
       "4  mmmm now you are speaking my language! (unfortunately, in militant diet mode-am seriously thinking of  competiton-so no tb i guess   \n",
       "\n",
       "     length  binnedLength  numWords   numCaps  numSpecial  numEmoticons  \\\n",
       "0  0.000679             6  0.208955  0.000000    0.000000      0.000000   \n",
       "1  0.418487            12  0.257812  0.093750    0.039062      0.007812   \n",
       "2 -0.095211             5  0.226415  0.037736    0.018868      0.000000   \n",
       "3  0.377392            11  0.188525  0.000000    0.024590      0.000000   \n",
       "4  0.432186            12  0.176923  0.030769    0.023077      0.000000   \n",
       "\n",
       "   numMultiExclamations  numMultiStopMarks     ...      \\\n",
       "0                   0.0           0.000000     ...       \n",
       "1                   0.0           0.000000     ...       \n",
       "2                   0.0           0.000000     ...       \n",
       "3                   0.0           0.008197     ...       \n",
       "4                   0.0           0.000000     ...       \n",
       "\n",
       "   mostSignificantSentimentScore  num_angerWords  num_anticipationWords  \\\n",
       "0                       0.200000        0.000000               0.000000   \n",
       "1                       0.416667        0.000000               0.030303   \n",
       "2                       0.000000        0.083333               0.000000   \n",
       "3                       0.000000        0.000000               0.086957   \n",
       "4                       0.000000        0.000000               0.000000   \n",
       "\n",
       "   num_disgustWords  num_fearWords  num_joyWords  num_sadnessWords  \\\n",
       "0          0.000000       0.000000      0.000000          0.000000   \n",
       "1          0.000000       0.000000      0.060606          0.000000   \n",
       "2          0.166667       0.083333      0.000000          0.000000   \n",
       "3          0.043478       0.000000      0.000000          0.043478   \n",
       "4          0.000000       0.000000      0.000000          0.000000   \n",
       "\n",
       "   num_surpriseWords  num_trustWords  linenumber  \n",
       "0           0.000000        0.000000           0  \n",
       "1           0.030303        0.060606           1  \n",
       "2           0.000000        0.083333           2  \n",
       "3           0.000000        0.043478           3  \n",
       "4           0.043478        0.000000           4  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['linenumber'] = train['Unnamed: 0']\n",
    "train = train.drop(columns=['Unnamed: 0'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_fe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>length</th>\n",
       "      <th>numWords</th>\n",
       "      <th>numCaps</th>\n",
       "      <th>numSpecial</th>\n",
       "      <th>numEmoticons</th>\n",
       "      <th>numMultiExclamations</th>\n",
       "      <th>numMultiStopMarks</th>\n",
       "      <th>numMultiQuestionMarks</th>\n",
       "      <th>numPositiveWords</th>\n",
       "      <th>...</th>\n",
       "      <th>mostSignificantSentimentScore</th>\n",
       "      <th>num_angerWords</th>\n",
       "      <th>num_anticipationWords</th>\n",
       "      <th>num_disgustWords</th>\n",
       "      <th>num_fearWords</th>\n",
       "      <th>num_joyWords</th>\n",
       "      <th>num_sadnessWords</th>\n",
       "      <th>num_surpriseWords</th>\n",
       "      <th>num_trustWords</th>\n",
       "      <th>linenumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>your still thinking?  cheer up buddy )</td>\n",
       "      <td>-0.194952</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forecast in sf for friday, may   degrees.</td>\n",
       "      <td>-0.167366</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.02381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>driving past tattered cover w/o stopping  promising myself to make ti ame next week.</td>\n",
       "      <td>0.122289</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wowzer! it is very windy. not good for my allergies multiExclamation</td>\n",
       "      <td>-0.091504</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>start of diet today  i think i have to face i will never get back down to kilos (unless i cut a leg off)</td>\n",
       "      <td>0.260220</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       Text  \\\n",
       "0                                                                    your still thinking?  cheer up buddy )   \n",
       "1                                                                 forecast in sf for friday, may   degrees.   \n",
       "2                      driving past tattered cover w/o stopping  promising myself to make ti ame next week.   \n",
       "3                                     wowzer! it is very windy. not good for my allergies multiExclamation    \n",
       "4  start of diet today  i think i have to face i will never get back down to kilos (unless i cut a leg off)   \n",
       "\n",
       "     length  numWords   numCaps  numSpecial  numEmoticons  \\\n",
       "0 -0.194952  0.210526  0.078947    0.052632       0.00000   \n",
       "1 -0.167366  0.238095  0.000000    0.071429       0.02381   \n",
       "2  0.122289  0.178571  0.023810    0.011905       0.00000   \n",
       "3 -0.091504  0.264151  0.056604    0.056604       0.00000   \n",
       "4  0.260220  0.250000  0.048077    0.019231       0.00000   \n",
       "\n",
       "   numMultiExclamations  numMultiStopMarks  numMultiQuestionMarks  \\\n",
       "0              0.000000                0.0                      0   \n",
       "1              0.000000                0.0                      0   \n",
       "2              0.000000                0.0                      0   \n",
       "3              0.018868                0.0                      0   \n",
       "4              0.000000                0.0                      0   \n",
       "\n",
       "   numPositiveWords     ...      mostSignificantSentimentScore  \\\n",
       "0          0.000000     ...                                0.0   \n",
       "1          0.000000     ...                                0.0   \n",
       "2          0.066667     ...                                0.2   \n",
       "3          0.071429     ...                               -0.7   \n",
       "4          0.000000     ...                                0.0   \n",
       "\n",
       "   num_angerWords  num_anticipationWords  num_disgustWords  num_fearWords  \\\n",
       "0             0.0               0.250000               0.0            0.0   \n",
       "1             0.0               0.100000               0.0            0.0   \n",
       "2             0.0               0.000000               0.0            0.0   \n",
       "3             0.0               0.071429               0.0            0.0   \n",
       "4             0.0               0.038462               0.0            0.0   \n",
       "\n",
       "   num_joyWords  num_sadnessWords  num_surpriseWords  num_trustWords  \\\n",
       "0      0.250000               0.0           0.125000        0.250000   \n",
       "1      0.000000               0.0           0.000000        0.100000   \n",
       "2      0.000000               0.0           0.000000        0.066667   \n",
       "3      0.071429               0.0           0.071429        0.071429   \n",
       "4      0.000000               0.0           0.000000        0.000000   \n",
       "\n",
       "   linenumber  \n",
       "0           0  \n",
       "1           1  \n",
       "2           2  \n",
       "3           3  \n",
       "4           4  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['linenumber'] = test['Unnamed: 0']\n",
    "test = test.drop(columns=['Unnamed: 0'])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment                        False\n",
       "Text                             False\n",
       "length                           False\n",
       "binnedLength                     False\n",
       "numWords                         False\n",
       "numCaps                          False\n",
       "numSpecial                       False\n",
       "numEmoticons                     False\n",
       "numMultiExclamations             False\n",
       "numMultiStopMarks                False\n",
       "numMultiQuestionMarks            False\n",
       "numPositiveWords                 False\n",
       "numNegativeWords                 False\n",
       "numNeutralWords                  False\n",
       "numSlangs                        False\n",
       "numElongated                     False\n",
       "mostSignificantSentimentScore    False\n",
       "num_angerWords                   False\n",
       "num_anticipationWords            False\n",
       "num_disgustWords                 False\n",
       "num_fearWords                    False\n",
       "num_joyWords                     False\n",
       "num_sadnessWords                 False\n",
       "num_surpriseWords                False\n",
       "num_trustWords                   False\n",
       "linenumber                       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text                              True\n",
       "length                           False\n",
       "numWords                          True\n",
       "numCaps                           True\n",
       "numSpecial                        True\n",
       "numEmoticons                      True\n",
       "numMultiExclamations              True\n",
       "numMultiStopMarks                 True\n",
       "numMultiQuestionMarks            False\n",
       "numPositiveWords                  True\n",
       "numNegativeWords                  True\n",
       "numNeutralWords                   True\n",
       "numSlangs                         True\n",
       "numElongated                      True\n",
       "mostSignificantSentimentScore    False\n",
       "num_angerWords                    True\n",
       "num_anticipationWords             True\n",
       "num_disgustWords                  True\n",
       "num_fearWords                     True\n",
       "num_joyWords                      True\n",
       "num_sadnessWords                  True\n",
       "num_surpriseWords                 True\n",
       "num_trustWords                    True\n",
       "linenumber                       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.Text = test.Text.fillna('EMPTY')\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text                             False\n",
       "length                           False\n",
       "numWords                         False\n",
       "numCaps                          False\n",
       "numSpecial                       False\n",
       "numEmoticons                     False\n",
       "numMultiExclamations             False\n",
       "numMultiStopMarks                False\n",
       "numMultiQuestionMarks            False\n",
       "numPositiveWords                 False\n",
       "numNegativeWords                 False\n",
       "numNeutralWords                  False\n",
       "numSlangs                        False\n",
       "numElongated                     False\n",
       "mostSignificantSentimentScore    False\n",
       "num_angerWords                   False\n",
       "num_anticipationWords            False\n",
       "num_disgustWords                 False\n",
       "num_fearWords                    False\n",
       "num_joyWords                     False\n",
       "num_sadnessWords                 False\n",
       "num_surpriseWords                False\n",
       "num_trustWords                   False\n",
       "linenumber                       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Ascii Chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there are some non-ascii chars in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True    30997\n",
      "Name: Text, dtype: int64\n",
      "True    6576\n",
      "Name: Text, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(train[\"Text\"].apply(lambda x: all(ord(c) < 128 for c in x))).Text.value_counts())\n",
    "print(pd.DataFrame(test[\"Text\"].apply(lambda x: all(ord(c) < 128 for c in x))).Text.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hobj = hunspell.Hunspell()\n",
    "# def fixNonAscii(doc):\n",
    "#     tokens = word_tokenize(doc)\n",
    "#     fixed = []\n",
    "#     for word in tokens:\n",
    "#         fixedWord = word\n",
    "#         if not all(ord(c) < 128 for c in word):\n",
    "#             sugs = hobj.suggest(word)\n",
    "#             if(len(sugs)>0):\n",
    "#                 fixedWord = sugs[0]\n",
    "#             else:\n",
    "#                 fixedWord = [\" \" if ord(i) > 128 else i for i in word][0]\n",
    "#         fixed.append(fixedWord)\n",
    "#     return \" \".join([word for word in fixed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train['Text'] = train['Text'].apply(lambda x: fixNonAscii(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test['Text'] = test['Text'].apply(lambda x: fixNonAscii(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of sentences in train is 13.\n",
      "Average word length of phrases in test is 13.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of sentences in train is {0:.0f}.'.format(np.mean(train['Text'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Text'].astype(str).apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Data Preparation for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "stemmer = SnowballStemmer('english')\n",
    "def applyStopWordAndStemming(data):\n",
    "    stopped_data = []\n",
    "    stemmed_data = []\n",
    "    for doc in data:\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered = [word for word in tokens if word not in stop_words]\n",
    "        stopped_data.append(\" \".join([word for word in filtered]))\n",
    "        stemmed = [stemmer.stem(word) for word in filtered]\n",
    "        stemmed_data.append(\" \".join([word for word in stemmed]))\n",
    "        \n",
    "    return (stopped_data,stemmed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWordAndStemming = applyStopWordAndStemming(train['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Text</th>\n",
       "      <th>length</th>\n",
       "      <th>binnedLength</th>\n",
       "      <th>numWords</th>\n",
       "      <th>numCaps</th>\n",
       "      <th>numSpecial</th>\n",
       "      <th>numEmoticons</th>\n",
       "      <th>numMultiExclamations</th>\n",
       "      <th>numMultiStopMarks</th>\n",
       "      <th>...</th>\n",
       "      <th>num_anticipationWords</th>\n",
       "      <th>num_disgustWords</th>\n",
       "      <th>num_fearWords</th>\n",
       "      <th>num_joyWords</th>\n",
       "      <th>num_sadnessWords</th>\n",
       "      <th>num_surpriseWords</th>\n",
       "      <th>num_trustWords</th>\n",
       "      <th>linenumber</th>\n",
       "      <th>TextwithoutStopWords</th>\n",
       "      <th>TextwithStemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>she had another song out recently but it did not get that much play</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>6</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>another song recently get much play</td>\n",
       "      <td>anoth song recent get much play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>wellity! i think i ama gonna clean my room | gay. i need to read a good book. sense and sensibility here we come! beat the lily</td>\n",
       "      <td>0.418487</td>\n",
       "      <td>12</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>1</td>\n",
       "      <td>wellity ! think ama gon na clean room | gay need read good book sense sensibility come ! beat lily</td>\n",
       "      <td>welliti ! think ama gon na clean room | gay need read good book sens sensibl come ! beat lili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>bug in damn cod system link. my map pack isnt working</td>\n",
       "      <td>-0.095211</td>\n",
       "      <td>5</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>2</td>\n",
       "      <td>bug damn cod system link map pack isnt working</td>\n",
       "      <td>bug damn cod system link map pack isnt work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>weeooow, i feel fat multiStop  i remember when i used to weigh that much  pfffffffffffffffffffffftttttttt long ti ame ago, hahaha</td>\n",
       "      <td>0.377392</td>\n",
       "      <td>11</td>\n",
       "      <td>0.188525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>3</td>\n",
       "      <td>weeooow feel fat multiStop remember used weigh much pfffffffffffffffffffffftttttttt long ti ame ago hahaha</td>\n",
       "      <td>weeooow feel fat multistop rememb use weigh much pfffffffffffffffffffffftttttttt long ti ame ago hahaha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>mmmm now you are speaking my language! (unfortunately, in militant diet mode-am seriously thinking of  competiton-so no tb i guess</td>\n",
       "      <td>0.432186</td>\n",
       "      <td>12</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.023077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>mmmm speaking language ! unfortunately militant diet mode-am seriously thinking competiton-so tb guess</td>\n",
       "      <td>mmmm speak languag ! unfortun milit diet mode-am serious think competiton-so tb guess</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  \\\n",
       "0          2   \n",
       "1          2   \n",
       "2          2   \n",
       "3          8   \n",
       "4          7   \n",
       "\n",
       "                                                                                                                                 Text  \\\n",
       "0                                                                 she had another song out recently but it did not get that much play   \n",
       "1     wellity! i think i ama gonna clean my room | gay. i need to read a good book. sense and sensibility here we come! beat the lily   \n",
       "2                                                                               bug in damn cod system link. my map pack isnt working   \n",
       "3   weeooow, i feel fat multiStop  i remember when i used to weigh that much  pfffffffffffffffffffffftttttttt long ti ame ago, hahaha   \n",
       "4  mmmm now you are speaking my language! (unfortunately, in militant diet mode-am seriously thinking of  competiton-so no tb i guess   \n",
       "\n",
       "     length  binnedLength  numWords   numCaps  numSpecial  numEmoticons  \\\n",
       "0  0.000679             6  0.208955  0.000000    0.000000      0.000000   \n",
       "1  0.418487            12  0.257812  0.093750    0.039062      0.007812   \n",
       "2 -0.095211             5  0.226415  0.037736    0.018868      0.000000   \n",
       "3  0.377392            11  0.188525  0.000000    0.024590      0.000000   \n",
       "4  0.432186            12  0.176923  0.030769    0.023077      0.000000   \n",
       "\n",
       "   numMultiExclamations  numMultiStopMarks  \\\n",
       "0                   0.0           0.000000   \n",
       "1                   0.0           0.000000   \n",
       "2                   0.0           0.000000   \n",
       "3                   0.0           0.008197   \n",
       "4                   0.0           0.000000   \n",
       "\n",
       "                                                    ...                                                     \\\n",
       "0                                                   ...                                                      \n",
       "1                                                   ...                                                      \n",
       "2                                                   ...                                                      \n",
       "3                                                   ...                                                      \n",
       "4                                                   ...                                                      \n",
       "\n",
       "   num_anticipationWords  num_disgustWords  num_fearWords  num_joyWords  \\\n",
       "0               0.000000          0.000000       0.000000      0.000000   \n",
       "1               0.030303          0.000000       0.000000      0.060606   \n",
       "2               0.000000          0.166667       0.083333      0.000000   \n",
       "3               0.086957          0.043478       0.000000      0.000000   \n",
       "4               0.000000          0.000000       0.000000      0.000000   \n",
       "\n",
       "   num_sadnessWords  num_surpriseWords  num_trustWords  linenumber  \\\n",
       "0          0.000000           0.000000        0.000000           0   \n",
       "1          0.000000           0.030303        0.060606           1   \n",
       "2          0.000000           0.000000        0.083333           2   \n",
       "3          0.043478           0.000000        0.043478           3   \n",
       "4          0.000000           0.043478        0.000000           4   \n",
       "\n",
       "                                                                                         TextwithoutStopWords  \\\n",
       "0                                                                         another song recently get much play   \n",
       "1          wellity ! think ama gon na clean room | gay need read good book sense sensibility come ! beat lily   \n",
       "2                                                              bug damn cod system link map pack isnt working   \n",
       "3  weeooow feel fat multiStop remember used weigh much pfffffffffffffffffffffftttttttt long ti ame ago hahaha   \n",
       "4      mmmm speaking language ! unfortunately militant diet mode-am seriously thinking competiton-so tb guess   \n",
       "\n",
       "                                                                                          TextwithStemming  \n",
       "0                                                                          anoth song recent get much play  \n",
       "1            welliti ! think ama gon na clean room | gay need read good book sens sensibl come ! beat lili  \n",
       "2                                                              bug damn cod system link map pack isnt work  \n",
       "3  weeooow feel fat multistop rememb use weigh much pfffffffffffffffffffffftttttttt long ti ame ago hahaha  \n",
       "4                    mmmm speak languag ! unfortun milit diet mode-am serious think competiton-so tb guess  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['TextwithoutStopWords'] = stopWordAndStemming[0]\n",
    "train['TextwithStemming'] = stopWordAndStemming[1]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWordAndStemming = applyStopWordAndStemming(test['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>length</th>\n",
       "      <th>numWords</th>\n",
       "      <th>numCaps</th>\n",
       "      <th>numSpecial</th>\n",
       "      <th>numEmoticons</th>\n",
       "      <th>numMultiExclamations</th>\n",
       "      <th>numMultiStopMarks</th>\n",
       "      <th>numMultiQuestionMarks</th>\n",
       "      <th>numPositiveWords</th>\n",
       "      <th>...</th>\n",
       "      <th>num_anticipationWords</th>\n",
       "      <th>num_disgustWords</th>\n",
       "      <th>num_fearWords</th>\n",
       "      <th>num_joyWords</th>\n",
       "      <th>num_sadnessWords</th>\n",
       "      <th>num_surpriseWords</th>\n",
       "      <th>num_trustWords</th>\n",
       "      <th>linenumber</th>\n",
       "      <th>TextwithoutStopWords</th>\n",
       "      <th>TextwithStemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>your still thinking?  cheer up buddy )</td>\n",
       "      <td>-0.194952</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>still thinking ? cheer buddy</td>\n",
       "      <td>still think ? cheer buddi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forecast in sf for friday, may   degrees.</td>\n",
       "      <td>-0.167366</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.02381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1</td>\n",
       "      <td>forecast sf friday may degrees</td>\n",
       "      <td>forecast sf friday may degre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>driving past tattered cover w/o stopping  promising myself to make ti ame next week.</td>\n",
       "      <td>0.122289</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>2</td>\n",
       "      <td>driving past tattered cover w/o stopping promising make ti ame next week</td>\n",
       "      <td>drive past tatter cover w/o stop promis make ti ame next week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wowzer! it is very windy. not good for my allergies multiExclamation</td>\n",
       "      <td>-0.091504</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>3</td>\n",
       "      <td>wowzer ! windy good allergies multiExclamation</td>\n",
       "      <td>wowzer ! windi good allergi multiexclam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>start of diet today  i think i have to face i will never get back down to kilos (unless i cut a leg off)</td>\n",
       "      <td>0.260220</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>start diet today think face never get back kilos unless cut leg</td>\n",
       "      <td>start diet today think face never get back kilo unless cut leg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       Text  \\\n",
       "0                                                                    your still thinking?  cheer up buddy )   \n",
       "1                                                                 forecast in sf for friday, may   degrees.   \n",
       "2                      driving past tattered cover w/o stopping  promising myself to make ti ame next week.   \n",
       "3                                     wowzer! it is very windy. not good for my allergies multiExclamation    \n",
       "4  start of diet today  i think i have to face i will never get back down to kilos (unless i cut a leg off)   \n",
       "\n",
       "     length  numWords   numCaps  numSpecial  numEmoticons  \\\n",
       "0 -0.194952  0.210526  0.078947    0.052632       0.00000   \n",
       "1 -0.167366  0.238095  0.000000    0.071429       0.02381   \n",
       "2  0.122289  0.178571  0.023810    0.011905       0.00000   \n",
       "3 -0.091504  0.264151  0.056604    0.056604       0.00000   \n",
       "4  0.260220  0.250000  0.048077    0.019231       0.00000   \n",
       "\n",
       "   numMultiExclamations  numMultiStopMarks  numMultiQuestionMarks  \\\n",
       "0              0.000000                0.0                      0   \n",
       "1              0.000000                0.0                      0   \n",
       "2              0.000000                0.0                      0   \n",
       "3              0.018868                0.0                      0   \n",
       "4              0.000000                0.0                      0   \n",
       "\n",
       "   numPositiveWords  \\\n",
       "0          0.000000   \n",
       "1          0.000000   \n",
       "2          0.066667   \n",
       "3          0.071429   \n",
       "4          0.000000   \n",
       "\n",
       "                                ...                                \\\n",
       "0                               ...                                 \n",
       "1                               ...                                 \n",
       "2                               ...                                 \n",
       "3                               ...                                 \n",
       "4                               ...                                 \n",
       "\n",
       "   num_anticipationWords  num_disgustWords  num_fearWords  num_joyWords  \\\n",
       "0               0.250000               0.0            0.0      0.250000   \n",
       "1               0.100000               0.0            0.0      0.000000   \n",
       "2               0.000000               0.0            0.0      0.000000   \n",
       "3               0.071429               0.0            0.0      0.071429   \n",
       "4               0.038462               0.0            0.0      0.000000   \n",
       "\n",
       "   num_sadnessWords  num_surpriseWords  num_trustWords  linenumber  \\\n",
       "0               0.0           0.125000        0.250000           0   \n",
       "1               0.0           0.000000        0.100000           1   \n",
       "2               0.0           0.000000        0.066667           2   \n",
       "3               0.0           0.071429        0.071429           3   \n",
       "4               0.0           0.000000        0.000000           4   \n",
       "\n",
       "                                                       TextwithoutStopWords  \\\n",
       "0                                              still thinking ? cheer buddy   \n",
       "1                                            forecast sf friday may degrees   \n",
       "2  driving past tattered cover w/o stopping promising make ti ame next week   \n",
       "3                            wowzer ! windy good allergies multiExclamation   \n",
       "4           start diet today think face never get back kilos unless cut leg   \n",
       "\n",
       "                                                 TextwithStemming  \n",
       "0                                       still think ? cheer buddi  \n",
       "1                                    forecast sf friday may degre  \n",
       "2   drive past tatter cover w/o stop promis make ti ame next week  \n",
       "3                         wowzer ! windi good allergi multiexclam  \n",
       "4  start diet today think face never get back kilo unless cut leg  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['TextwithoutStopWords'] = stopWordAndStemming[0]\n",
    "test['TextwithStemming'] = stopWordAndStemming[1]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train.drop(columns=['sentiment'])\n",
    "y = train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split( \n",
    "              X, y, test_size = 0.3, random_state = 42, stratify = y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\n",
    "\n",
    "def generateCsr(train, cv, test, colname):\n",
    "    train_vectorized = vectorizer.fit_transform(train[colname])\n",
    "    cv_vectorized = vectorizer.transform(cv[colname])\n",
    "    test_vectorized = vectorizer.transform(test[colname])\n",
    "    \n",
    "    return (train_vectorized, cv_vectorized, test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csrs = generateCsr(X_train, X_cv, test, 'Text')\n",
    "train_vectorized = csrs[0]\n",
    "cv_vectorized = csrs[1]\n",
    "test_vectorized = csrs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csrs = generateCsr(X_train, X_cv, test, 'TextwithoutStopWords')\n",
    "train_withoutstop_vectorized = csrs[0]\n",
    "cv_withoutstop_vectorized = csrs[1]\n",
    "test_withoutstop_vectorized = csrs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csrs = generateCsr(X_train, X_cv, test, 'TextwithStemming')\n",
    "train_withstemming_vectorized = csrs[0]\n",
    "cv_withstemming_vectorized = csrs[1]\n",
    "test_withstemming_vectorized = csrs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vectorized_lda = lda.fit_transform(train_vectorized)\n",
    "cv_vectorized_lda = lda.transform(cv_vectorized)\n",
    "test_vectorized_lda = lda.transform(test_vectorized)\n",
    "train_withoutstop_vectorized_lda = lda.fit_transform(train_withoutstop_vectorized)\n",
    "cv_withoutstop_vectorized_lda = lda.transform(cv_withoutstop_vectorized)\n",
    "test_withoutstop_vectorized_lda = lda.transform(test_withoutstop_vectorized)\n",
    "train_withstemming_vectorized_lda = lda.fit_transform(train_withstemming_vectorized)\n",
    "cv_withstemming_vectorized_lda = lda.transform(cv_withstemming_vectorized)\n",
    "test_withstemming_vectorized_lda = lda.transform(test_withstemming_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vectorized = hstack((train_vectorized, train_vectorized_lda)).tocsr()\n",
    "cv_vectorized = hstack((cv_vectorized, cv_vectorized_lda)).tocsr()\n",
    "test_vectorized = hstack((test_vectorized, test_vectorized_lda)).tocsr()\n",
    "train_withoutstop_vectorized = hstack((train_withoutstop_vectorized, train_withoutstop_vectorized_lda)).tocsr()\n",
    "cv_withoutstop_vectorized = hstack((cv_withoutstop_vectorized, cv_withoutstop_vectorized_lda)).tocsr()\n",
    "test_withoutstop_vectorized = hstack((test_withoutstop_vectorized, test_withoutstop_vectorized_lda)).tocsr()\n",
    "train_withstemming_vectorized = hstack((train_withstemming_vectorized, train_withstemming_vectorized_lda)).tocsr()\n",
    "cv_withstemming_vectorized = hstack((cv_withstemming_vectorized, cv_withstemming_vectorized_lda)).tocsr()\n",
    "test_withstemming_vectorized = hstack((test_withstemming_vectorized, test_withstemming_vectorized_lda)).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Generated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainGenFeatures = X_train.drop(columns=['Text','TextwithoutStopWords','TextwithStemming','linenumber'])\n",
    "cvGenFeatures = X_cv.drop(columns=['Text','TextwithoutStopWords','TextwithStemming','linenumber'])\n",
    "testGenFeatures = test.drop(columns=['Text','TextwithoutStopWords','TextwithStemming','linenumber'])\n",
    "def addGenFeaturesToCsr(csr, genFeatures):\n",
    "    for col in genFeatures.columns:\n",
    "        csr = hstack((csr, np.array(genFeatures[col])[:,None])).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addGenFeaturesToCsr(train_vectorized, trainGenFeatures)\n",
    "addGenFeaturesToCsr(cv_vectorized, cvGenFeatures)\n",
    "addGenFeaturesToCsr(test_vectorized, testGenFeatures)\n",
    "addGenFeaturesToCsr(train_withoutstop_vectorized, trainGenFeatures)\n",
    "addGenFeaturesToCsr(cv_withoutstop_vectorized, cvGenFeatures)\n",
    "addGenFeaturesToCsr(test_withoutstop_vectorized, testGenFeatures)\n",
    "addGenFeaturesToCsr(train_withstemming_vectorized, trainGenFeatures)\n",
    "addGenFeaturesToCsr(cv_withstemming_vectorized, cvGenFeatures)\n",
    "addGenFeaturesToCsr(test_withstemming_vectorized, testGenFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21697, 148487)\n",
      "(9300, 148487)\n",
      "(6576, 148487)\n",
      "(21697, 134118)\n",
      "(9300, 134118)\n",
      "(6576, 134118)\n",
      "(21697, 124719)\n",
      "(9300, 124719)\n",
      "(6576, 124719)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectorized.shape)\n",
    "print(cv_vectorized.shape)\n",
    "print(test_vectorized.shape)\n",
    "print(train_withoutstop_vectorized.shape)\n",
    "print(cv_withoutstop_vectorized.shape)\n",
    "print(test_withoutstop_vectorized.shape)\n",
    "print(train_withstemming_vectorized.shape)\n",
    "print(cv_withstemming_vectorized.shape)\n",
    "print(test_withstemming_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainModel(estimator, params, train, y, cv=5):\n",
    "    ovr = OneVsRestClassifier(estimator)\n",
    "    ovrCV = GridSearchCV(ovr, params, n_jobs=-1, scoring='f1_micro',cv=cv)\n",
    "    ovrCV.fit(train, y)\n",
    "    return ovrCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(penalty='l1', dual=False, solver='liblinear', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {  \n",
    "    \"estimator__max_iter\": [200, 210, 220],\n",
    "    \"estimator__C\": [1.2, 1.3, 1.4, 1.5, 1.6, 1.7]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "ovrCV = trainModel(logreg, params, train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3720329999539107"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovrCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=210, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=0, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovrCV.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvPredicted = ovrCV.best_estimator_.predict(cv_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3749462365591398"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_cv, cvPredicted, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization without Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.9 s, sys: 336 ms, total: 10.2 s\n",
      "Wall time: 4min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "ovrCV = trainModel(logreg, params, train_withoutstop_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36442826197170114"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovrCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.6, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=200, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=0, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovrCV.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvPredicted = ovrCV.best_estimator_.predict(cv_withoutstop_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36870967741935484"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_cv, cvPredicted, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization with Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 354 ms, total: 11.2 s\n",
      "Wall time: 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "ovrCV = trainModel(logreg, params, train_withstemming_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36654837074249896"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovrCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.7, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=210, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=0, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovrCV.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvPredicted = ovrCV.best_estimator_.predict(cv_withstemming_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3695698924731183"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_cv, cvPredicted, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = LinearSVC(dual=False, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {  \n",
    "    \"estimator__C\": [1.2, 1.3, 1.4, 1.5, 1.6, 1.7]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 769 ms, total: 1min 19s\n",
      "Wall time: 8min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ovrCV = trainModel(svc, params, train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3565183727457496"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovrCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovrCV.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvPredicted = ovrCV.best_estimator_.predict(cv_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37387096774193557"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_cv, cvPredicted, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization without Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 s, sys: 253 ms, total: 20.2 s\n",
      "Wall time: 57.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "ovrCV = trainModel(svc, params, train_withoutstop_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36197051327547825"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovrCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovrCV.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvPredicted = ovrCV.best_estimator_.predict(cv_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37387096774193557"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_cv, cvPredicted, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization with Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 s, sys: 220 ms, total: 18.2 s\n",
      "Wall time: 50.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "ovrCV = trainModel(svc, params, train_withstemming_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3647772365067587"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovrCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovrCV.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvPredicted = ovrCV.best_estimator_.predict(cv_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37387096774193557"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_cv, cvPredicted, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_to_left = st.beta(10, 1)  \n",
    "from_zero_positive = st.expon(0, 50)\n",
    "\n",
    "params = {  \n",
    "    \"n_estimators\": st.randint(3, 40),\n",
    "    \"max_depth\": st.randint(3, 40),\n",
    "    \"learning_rate\": st.uniform(0.05, 0.4),\n",
    "    \"colsample_bytree\": one_to_left,\n",
    "    \"subsample\": one_to_left,\n",
    "    \"gamma\": st.uniform(0, 10),\n",
    "    'reg_alpha': from_zero_positive,\n",
    "    \"min_child_weight\": from_zero_positive,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = RandomizedSearchCV(xgb, params, n_jobs=-1, scoring='f1_micro',cv=3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amangal/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 416 ms, total: 1min 30s\n",
      "Wall time: 6min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "          estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a5f150dd8>, 'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a5f150550>, 'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a13d919e8>, 'colsample_bytree': <sc...5f150978>, 'min_child_weight': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a5f150978>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gs.fit(train_vectorized, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3361615640223247"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.9459548282105027, gamma=1.1784670611051662,\n",
       "       learning_rate=0.20797652959184654, max_delta_step=0, max_depth=7,\n",
       "       min_child_weight=9.524671323541103, missing=None, n_estimators=38,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=29.992879554845764, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=0.8911238902249374)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvPredicted = gs.best_estimator_.predict(cv_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37387096774193557"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_cv, cvPredicted, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization without Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "gs.fit(train_withoutstop_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.380133561312385"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.4398123508830265, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=204,\n",
       "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvPredicted = gs.best_estimator_.predict(cv_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37387096774193557"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_cv, cvPredicted, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization with Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "gs.fit(train_withstemming_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.380133561312385"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.4398123508830265, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=204,\n",
       "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvPredicted = gs.best_estimator_.predict(cv_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37387096774193557"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_cv, cvPredicted, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {  \n",
    "    \"n_neighbors\": [3,5,10,20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knnCV = GridSearchCV(knn, params, n_jobs=-1, scoring='f1_micro', cv=3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [3, 5, 10, 20]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='f1_micro',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnCV.fit(train_withstemming_vectorized, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24289447365874117"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnCV.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvPredicted = knnCV.best_estimator_.predict(cv_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37387096774193557"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_cv, cvPredicted, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(10,10),max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amangal/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(train_withstemming_vectorized,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvPredicted = mlp.predict(cv_withstemming_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29075268817204303"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_cv, cvPredicted, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin = ovrCV.predict(test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  3,  3, ...,  2,  2, 10])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(fin).to_csv(\"predictionsOvr_1022_0718.dat\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
